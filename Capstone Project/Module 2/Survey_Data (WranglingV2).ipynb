{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "649b575a",
   "metadata": {},
   "source": [
    "# **DATA WRANGLING LAB**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b87def8c",
   "metadata": {},
   "source": [
    "#### IMPORT REQUIRED LIBRARIES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e2d226b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3543389",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c9a93a5",
   "metadata": {},
   "source": [
    "#### LOAD DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "25a71e9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_url = \"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/n01PQ9pSmiRX6520flujwQ/survey-data.csv\"\n",
    "df = pd.read_csv(dataset_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67d76411",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc58065c",
   "metadata": {},
   "source": [
    "#### EXPLORE DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3c7c0d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Column Data Types\n",
    "print(f\"Data types of all columns:\\n{df.dtypes}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9acbfbd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Counts of all columns\n",
    "print(f\"Counts of all columns:\\n{df.count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45f99d16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Missing values counts\n",
    "print(f\"Number of missing values:\\n{df.isna().sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "15be9629",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Statistical Summary:\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ResponseId</th>\n",
       "      <th>CompTotal</th>\n",
       "      <th>WorkExp</th>\n",
       "      <th>JobSatPoints_1</th>\n",
       "      <th>JobSatPoints_4</th>\n",
       "      <th>JobSatPoints_5</th>\n",
       "      <th>JobSatPoints_6</th>\n",
       "      <th>JobSatPoints_7</th>\n",
       "      <th>JobSatPoints_8</th>\n",
       "      <th>JobSatPoints_9</th>\n",
       "      <th>JobSatPoints_10</th>\n",
       "      <th>JobSatPoints_11</th>\n",
       "      <th>ConvertedCompYearly</th>\n",
       "      <th>JobSat</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>65437.000000</td>\n",
       "      <td>3.374000e+04</td>\n",
       "      <td>29658.000000</td>\n",
       "      <td>29324.000000</td>\n",
       "      <td>29393.000000</td>\n",
       "      <td>29411.000000</td>\n",
       "      <td>29450.000000</td>\n",
       "      <td>29448.00000</td>\n",
       "      <td>29456.000000</td>\n",
       "      <td>29456.000000</td>\n",
       "      <td>29450.000000</td>\n",
       "      <td>29445.000000</td>\n",
       "      <td>2.343500e+04</td>\n",
       "      <td>29126.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>32719.000000</td>\n",
       "      <td>2.963841e+145</td>\n",
       "      <td>11.466957</td>\n",
       "      <td>18.581094</td>\n",
       "      <td>7.522140</td>\n",
       "      <td>10.060857</td>\n",
       "      <td>24.343232</td>\n",
       "      <td>22.96522</td>\n",
       "      <td>20.278165</td>\n",
       "      <td>16.169432</td>\n",
       "      <td>10.955713</td>\n",
       "      <td>9.953948</td>\n",
       "      <td>8.615529e+04</td>\n",
       "      <td>6.935041</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>18890.179119</td>\n",
       "      <td>5.444117e+147</td>\n",
       "      <td>9.168709</td>\n",
       "      <td>25.966221</td>\n",
       "      <td>18.422661</td>\n",
       "      <td>21.833836</td>\n",
       "      <td>27.089360</td>\n",
       "      <td>27.01774</td>\n",
       "      <td>26.108110</td>\n",
       "      <td>24.845032</td>\n",
       "      <td>22.906263</td>\n",
       "      <td>21.775652</td>\n",
       "      <td>1.867570e+05</td>\n",
       "      <td>2.088259</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>16360.000000</td>\n",
       "      <td>6.000000e+04</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.271200e+04</td>\n",
       "      <td>6.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>32719.000000</td>\n",
       "      <td>1.100000e+05</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>15.00000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6.500000e+04</td>\n",
       "      <td>7.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>49078.000000</td>\n",
       "      <td>2.500000e+05</td>\n",
       "      <td>16.000000</td>\n",
       "      <td>22.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>30.000000</td>\n",
       "      <td>30.00000</td>\n",
       "      <td>25.000000</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>1.079715e+05</td>\n",
       "      <td>8.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>65437.000000</td>\n",
       "      <td>1.000000e+150</td>\n",
       "      <td>50.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.00000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>1.625660e+07</td>\n",
       "      <td>10.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         ResponseId      CompTotal       WorkExp  JobSatPoints_1  \\\n",
       "count  65437.000000   3.374000e+04  29658.000000    29324.000000   \n",
       "mean   32719.000000  2.963841e+145     11.466957       18.581094   \n",
       "std    18890.179119  5.444117e+147      9.168709       25.966221   \n",
       "min        1.000000   0.000000e+00      0.000000        0.000000   \n",
       "25%    16360.000000   6.000000e+04      4.000000        0.000000   \n",
       "50%    32719.000000   1.100000e+05      9.000000       10.000000   \n",
       "75%    49078.000000   2.500000e+05     16.000000       22.000000   \n",
       "max    65437.000000  1.000000e+150     50.000000      100.000000   \n",
       "\n",
       "       JobSatPoints_4  JobSatPoints_5  JobSatPoints_6  JobSatPoints_7  \\\n",
       "count    29393.000000    29411.000000    29450.000000     29448.00000   \n",
       "mean         7.522140       10.060857       24.343232        22.96522   \n",
       "std         18.422661       21.833836       27.089360        27.01774   \n",
       "min          0.000000        0.000000        0.000000         0.00000   \n",
       "25%          0.000000        0.000000        0.000000         0.00000   \n",
       "50%          0.000000        0.000000       20.000000        15.00000   \n",
       "75%          5.000000       10.000000       30.000000        30.00000   \n",
       "max        100.000000      100.000000      100.000000       100.00000   \n",
       "\n",
       "       JobSatPoints_8  JobSatPoints_9  JobSatPoints_10  JobSatPoints_11  \\\n",
       "count    29456.000000    29456.000000     29450.000000     29445.000000   \n",
       "mean        20.278165       16.169432        10.955713         9.953948   \n",
       "std         26.108110       24.845032        22.906263        21.775652   \n",
       "min          0.000000        0.000000         0.000000         0.000000   \n",
       "25%          0.000000        0.000000         0.000000         0.000000   \n",
       "50%         10.000000        5.000000         0.000000         0.000000   \n",
       "75%         25.000000       20.000000        10.000000        10.000000   \n",
       "max        100.000000      100.000000       100.000000       100.000000   \n",
       "\n",
       "       ConvertedCompYearly        JobSat  \n",
       "count         2.343500e+04  29126.000000  \n",
       "mean          8.615529e+04      6.935041  \n",
       "std           1.867570e+05      2.088259  \n",
       "min           1.000000e+00      0.000000  \n",
       "25%           3.271200e+04      6.000000  \n",
       "50%           6.500000e+04      7.000000  \n",
       "75%           1.079715e+05      8.000000  \n",
       "max           1.625660e+07     10.000000  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Statistical summary\n",
    "print(\"Statistical Summary:\\n\")\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b738560",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b14d54b",
   "metadata": {},
   "source": [
    "#### Identifying Inconsistencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d6fdb50d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique countries: 182\n"
     ]
    }
   ],
   "source": [
    "# See how many entries there are\n",
    "print(f\"Number of unique countries: {df['Country'].nunique()}\")\n",
    "\n",
    "# Print all unique countries sorted alphabetically to spot typos\n",
    "# We convert to list to see the full output\n",
    "countries = sorted(df['Country'].dropna().unique().astype(str))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a10d8f52",
   "metadata": {},
   "source": [
    "#### Fix Formatting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0e9a4eb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert everything to title case\n",
    "df['Country'] = df['Country'].str.title()\n",
    "\n",
    "# Strip whitespace (removes invisible spaces at start/end)\n",
    "df['Country'] = df['Country'].str.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15b413fd",
   "metadata": {},
   "source": [
    "#### Standardize entries in `Country` column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ed4d392f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 24 non-standard country names\n"
     ]
    }
   ],
   "source": [
    "# Let's use pycountry libraries to create a valid country names\n",
    "import pycountry\n",
    "\n",
    "# Store valid countries names to valid_countries variable\n",
    "valid_countries = {country.name for country in pycountry.countries}\n",
    "\n",
    "# Filter out inconsistent countries name\n",
    "suspects = df[~df['Country'].isin(valid_countries)]['Country'].unique()\n",
    "\n",
    "print(f\"Found {len(suspects)} non-standard country names\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c8aceaa0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Country cleanup complete.\n"
     ]
    }
   ],
   "source": [
    "# 1. The Dictionary of fixes\n",
    "country_fix = {\n",
    "    'United States Of America': 'United States',\n",
    "    'United Kingdom Of Great Britain And Northern Ireland': 'United Kingdom',\n",
    "    'Republic Of Korea': 'South Korea',\n",
    "    'Democratic People\\'S Republic Of Korea': 'North Korea',\n",
    "    'Czech Republic': 'Czechia',          # Official short name update\n",
    "    'Swaziland': 'Eswatini',              # Country changed its name\n",
    "    'Turkey': 'Turkey',                   # Pycountry expects \"TÃ¼rkiye\", but Turkey is fine for English analysis\n",
    "    'Hong Kong (S.A.R.)': 'Hong Kong',\n",
    "    'Mainland China': 'China',\n",
    "    'Taiwan': 'Taiwan',                   # Pycountry often expects \"Taiwan, Province of China\"\n",
    "    \n",
    "    # Fixing the Truncated \"...\" entries\n",
    "    'Iran, Islamic Republic Of...': 'Iran',\n",
    "    'Venezuela, Bolivarian Republic Of...': 'Venezuela',\n",
    "    'Micronesia, Federated States Of...': 'Micronesia',\n",
    "    'Congo, Republic Of The...': 'Republic of the Congo'\n",
    "}\n",
    "\n",
    "# 2. Apply the map\n",
    "df['Country'] = df['Country'].replace(country_fix)\n",
    "\n",
    "# 3. Handle the \"Not a Country\" stuff\n",
    "# Drop rows where country is 'Nomadic' or missing (nan)\n",
    "df = df[df['Country'] != 'Nomadic']\n",
    "df = df.dropna(subset=['Country'])\n",
    "\n",
    "print(\"Country cleanup complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90997325",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28d1f7eb",
   "metadata": {},
   "source": [
    "#### ENCODING CATEGORICAL VARIABLES"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6da1fa4f",
   "metadata": {},
   "source": [
    "We'll encode `employment` column using one-hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b4c4a286",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The simple way to encode the 'Employment' column\n",
    "# This will remove the original 'Employment' column and replace it with the new ones.\n",
    "df_encoded = pd.get_dummies(df, columns=['Employment'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "380ff5f7",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32a29b46",
   "metadata": {},
   "source": [
    "#### HANDLING MISSING VALUES"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99eded69",
   "metadata": {},
   "source": [
    "Handle missing value in `RemoteWork` column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5e13cef6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AINextMuch less integrated       57814\n",
      "AINextLess integrated            56631\n",
      "AINextNo change                  46713\n",
      "AINextMuch more integrated       45826\n",
      "EmbeddedAdmired                  42504\n",
      "EmbeddedWantToWorkWith           41662\n",
      "EmbeddedHaveWorkedWith           37218\n",
      "ConvertedCompYearly              35456\n",
      "AIToolNot interested in Using    35268\n",
      "AINextMore integrated            35090\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Identify columns with most missing values\n",
    "print(df.isna().sum().sort_values(ascending=False).nlargest(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4a448c7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of missing Remote Work data: 0\n"
     ]
    }
   ],
   "source": [
    "# Impute missing values for RemoteWork column\n",
    "freq_rw = df['RemoteWork'].mode()[0]\n",
    "\n",
    "df['RemoteWork'] = df['RemoteWork'].fillna(freq_rw)\n",
    "\n",
    "# Verify\n",
    "print(f\"Number of missing Remote Work data: {df['RemoteWork'].isna().sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b68f543",
   "metadata": {},
   "source": [
    "Handle missing values in `ConvertedCompYearly` column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "210b33f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of missing Annual Compensation data: 35456\n"
     ]
    }
   ],
   "source": [
    "# ---Handle missing value in 'ConvertedCompYearly'---\n",
    "\n",
    "# First let's count the missing values\n",
    "print(f\"Number of missing Annual Compensation data: {df['ConvertedCompYearly'].isna().sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38635ace",
   "metadata": {},
   "source": [
    "Since the number of missing values is too high (more than 50%) to be replaced by median or mean. It's better to create a new clean dataframe by dropping missing values in this column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fbeafb6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of missing values: 0\n",
      "Shape of new comp dataframe: (23431, 114)\n"
     ]
    }
   ],
   "source": [
    "comp_df = df.dropna(subset=['ConvertedCompYearly']).copy()\n",
    "\n",
    "print(f\"Number of missing values: {comp_df['ConvertedCompYearly'].isna().sum()}\")\n",
    "print(f\"Shape of new comp dataframe: {comp_df.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d267efc",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acfe48f7",
   "metadata": {},
   "source": [
    "#### FEATURE SCALING AND TRANSFORMATION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1af805e",
   "metadata": {},
   "source": [
    "Before normalize the data, we'll remove any outliers first\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b62f3220",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of new clean dataframe: (22453, 114)\n"
     ]
    }
   ],
   "source": [
    "# ---REMOVE OUTLIERS USING IQR---\n",
    "Q1 = comp_df['ConvertedCompYearly'].quantile(0.25)\n",
    "Q3 = comp_df['ConvertedCompYearly'].quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "\n",
    "clean_comp = comp_df[(comp_df['ConvertedCompYearly'] >= Q1 - 1.5 * IQR) &\n",
    "                     (comp_df['ConvertedCompYearly'] <= Q3 + 1.5 * IQR)].copy()\n",
    "\n",
    "print(f\"Shape of new clean dataframe: {clean_comp.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70145422",
   "metadata": {},
   "source": [
    "Apply Min-Max scaling to normalize `ConvertedCompYearly` column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b70551e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_comp['ConvertedCompYearly_MinMax'] = ((clean_comp['ConvertedCompYearly'] - clean_comp['ConvertedCompYearly'].min()) /\n",
    "                                            (clean_comp['ConvertedCompYearly'].max() - clean_comp['ConvertedCompYearly'].min()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5923816f",
   "metadata": {},
   "source": [
    "Log-transform the ConvertedCompYearly column to reduce skewness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c47e76e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Skewness: 0.735462597159766\n",
      "Log Transformed Skewness: -2.5160117203372248\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Transform to log\n",
    "clean_comp['ConvertedCompYearly_Log'] = np.log1p(clean_comp['ConvertedCompYearly'])\n",
    "\n",
    "print(f\"Original Skewness: {clean_comp['ConvertedCompYearly'].skew()}\")\n",
    "print(f\"Log Transformed Skewness: {clean_comp['ConvertedCompYearly_Log'].skew()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20ffd20e",
   "metadata": {},
   "source": [
    "Seems like **log transformation** is too powerful for the data so it makes the skewness worst. Let's try using **square root**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e2600c84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Skewness: 0.735462597159766\n",
      "Square root Skewness: -0.1828742451946956\n"
     ]
    }
   ],
   "source": [
    "clean_comp['ConvertedCompYearly_sqrt'] = np.sqrt(clean_comp['ConvertedCompYearly'])\n",
    "\n",
    "print(f\"Original Skewness: {clean_comp['ConvertedCompYearly'].skew()}\")\n",
    "print(f\"Square root Skewness: {clean_comp['ConvertedCompYearly_sqrt'].skew()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dcb98ab",
   "metadata": {},
   "source": [
    "**Square Roots** works better for this data skewness, let's keep this result."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4960a5a9",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c00953b0",
   "metadata": {},
   "source": [
    "#### FEATURE ENGINEERING"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2051addf",
   "metadata": {},
   "source": [
    "Create a new column `ExperienceLevel` based on the `YearsCodePro` column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "27ae30ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>YearsCodePro</th>\n",
       "      <th>ExperienceLevel</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>59908</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54015</th>\n",
       "      <td>6.0</td>\n",
       "      <td>Senior</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56487</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37326</th>\n",
       "      <td>10.0</td>\n",
       "      <td>Senior</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46944</th>\n",
       "      <td>4.0</td>\n",
       "      <td>Mid-Level</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45874</th>\n",
       "      <td>8.0</td>\n",
       "      <td>Senior</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50533</th>\n",
       "      <td>4.0</td>\n",
       "      <td>Mid-Level</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14148</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15249</th>\n",
       "      <td>3.0</td>\n",
       "      <td>Mid-Level</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2563</th>\n",
       "      <td>35.0</td>\n",
       "      <td>Expert</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       YearsCodePro ExperienceLevel\n",
       "59908           NaN             NaN\n",
       "54015           6.0          Senior\n",
       "56487           NaN             NaN\n",
       "37326          10.0          Senior\n",
       "46944           4.0       Mid-Level\n",
       "45874           8.0          Senior\n",
       "50533           4.0       Mid-Level\n",
       "14148           NaN             NaN\n",
       "15249           3.0       Mid-Level\n",
       "2563           35.0          Expert"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert column to numerical type\n",
    "df['YearsCodePro'] = pd.to_numeric(df['YearsCodePro'], errors='coerce')\n",
    "\n",
    "# --Define Logic---\n",
    "# Bins: 0-2 (Junior), 3-5 (Mid), 6-10 (Senior), 11+ (Expert)\n",
    "bins = [-1, 2, 5, 10, 100]\n",
    "labels = ['Junior', 'Mid-Level', 'Senior', 'Expert']\n",
    "\n",
    "\n",
    "# Create 'ExperienceLevel' column\n",
    "df['ExperienceLevel'] = pd.cut(df['YearsCodePro'], bins=bins, labels=labels)\n",
    "\n",
    "# Check the results\n",
    "df[['YearsCodePro', 'ExperienceLevel']].sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d4fea5a",
   "metadata": {},
   "source": [
    "#### **Summary**\n",
    "\n",
    "I have been practicing:\n",
    "\n",
    "* Explored the dataset to identify inconsistencies and missing values.\n",
    "\n",
    "* Encoded categorical variables for analysis.\n",
    "\n",
    "* Handled missing values using imputation techniques.\n",
    "\n",
    "* Normalized and transformed numerical data to prepare it for analysis.\n",
    "\n",
    "* Engineered a new feature to enhance data interpretation."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "practice_session",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
